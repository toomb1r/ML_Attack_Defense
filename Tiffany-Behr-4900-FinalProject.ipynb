{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "203e95b6-7f52-4533-8c7b-326b72ebe7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset\n",
    "#https://www.kaggle.com/datasets/iamsouravbanerjee/animal-image-dataset-90-different-animals\n",
    "#CNN code for animal classification (different dataset)\n",
    "#https://www.kaggle.com/code/mehmetlaudatekman/pytorch-animal-face-classification-cnns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54575035-ec1a-4962-bf0f-887cca4a48a7",
   "metadata": {},
   "source": [
    "# Load and process data for image classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e219129d-68c0-4ff6-9c3d-5e541db4cf21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# * We'll use this class to control the things.\n",
    "class InvalidDatasetException(Exception):\n",
    "    \n",
    "    def __init__(self,len_of_paths,len_of_labels):\n",
    "        super().__init__(\n",
    "            f\"Number of paths ({len_of_paths}) is not compatible with number of labels ({len_of_labels})\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "88991385-0d5d-464e-972b-50e46e64336a",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "21726d53-9866-4142-aa8d-4a973f325674",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnimalDataset(Dataset):\n",
    "    \n",
    "    def __init__(self,img_paths,img_labels,size_of_images):\n",
    "        self.img_paths = img_paths\n",
    "        self.img_labels = img_labels\n",
    "        self.size_of_images = size_of_images\n",
    "        if len(self.img_paths) != len(self.img_labels):\n",
    "            raise InvalidDatasetException(self.img_paths,self.img_labels)\n",
    "        \n",
    "    \n",
    "    # We need to override __len__ special method\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "    \n",
    "    # Also we need to override __getitem__ special method\n",
    "    # This method should return the image and its label from index given.\n",
    "    def __getitem__(self,index):\n",
    "        PIL_IMAGE = Image.open(self.img_paths[index]).resize(self.size_of_images)\n",
    "        # In pytorch we use torch tensors, ToTensor transform transforms the PIL image \n",
    "        # to Torch tensor.\n",
    "        TENSOR_IMAGE = transform(PIL_IMAGE)\n",
    "        label = self.img_labels[index]\n",
    "        \n",
    "        return TENSOR_IMAGE,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "701cc3c6-474d-4435-ae4e-37c385bff5b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111\n",
      "111\n"
     ]
    }
   ],
   "source": [
    "paths = []\n",
    "labels = []\n",
    "label_map = {0:\"Cat\",\n",
    "             1:\"Tiger\",\n",
    "             2:\"Dog\",\n",
    "             3:\"Wolf\"\n",
    "            }\n",
    "\n",
    "for cat_path in \"/data/animals/animals/cat/*\":\n",
    "    paths.append(cat_path)\n",
    "    labels.append(0)\n",
    "    \n",
    "for tiger_path in \"/data/animals/animals/tiger/*\":\n",
    "    paths.append(tiger_path)\n",
    "    labels.append(1)\n",
    "    \n",
    "for dog_path in \"/data/animals/animals/dog/*\":\n",
    "    paths.append(dog_path)\n",
    "    labels.append(2)\n",
    "\n",
    "for wolf_path in \"/data/animals/animals/wolf/*\":\n",
    "    paths.append(wolf_path)\n",
    "    labels.append(2)\n",
    "    \n",
    "print(len(paths))\n",
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "af01ff07-65d8-4a10-adfb-f4d149510bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = AnimalDataset(paths,labels,(250,250))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1db10c3c-7d06-4da2-9d53-b5f33b79640f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train samples:  88\n",
      "Number of test samples:  23\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# dataset_indices = [0,1,2,3,..len(dataset)-1]\n",
    "dataset_indices = list(range(0,len(dataset)))\n",
    "\n",
    "train_indices,test_indices = train_test_split(dataset_indices,test_size=0.2,random_state=42)\n",
    "print(\"Number of train samples: \",len(train_indices))\n",
    "print(\"Number of test samples: \",len(test_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e3e7f22c-4a54-4ab3-9d11-96c38256d460",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "test_sampler = SubsetRandomSampler(test_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "72e391da-94ed-4196-af0c-63c5315bf360",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, \n",
    "                                           sampler=train_sampler)\n",
    "validation_loader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE,\n",
    "                                                sampler=test_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2c4da4bb-0fb1-4167-85fb-f87b7e6365ef",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'd'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape\n",
      "Cell \u001b[0;32mIn[18], line 18\u001b[0m, in \u001b[0;36mAnimalDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m,index):\n\u001b[0;32m---> 18\u001b[0m     PIL_IMAGE \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimg_paths\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mresize(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize_of_images)\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;66;03m# In pytorch we use torch tensors, ToTensor transform transforms the PIL image \u001b[39;00m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;66;03m# to Torch tensor.\u001b[39;00m\n\u001b[1;32m     21\u001b[0m     TENSOR_IMAGE \u001b[38;5;241m=\u001b[39m transform(PIL_IMAGE)\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/PIL/Image.py:2953\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   2950\u001b[0m     filename \u001b[38;5;241m=\u001b[39m fp\n\u001b[1;32m   2952\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename:\n\u001b[0;32m-> 2953\u001b[0m     fp \u001b[38;5;241m=\u001b[39m \u001b[43mbuiltins\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2954\u001b[0m     exclusive_fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   2956\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'd'"
     ]
    }
   ],
   "source": [
    "#dataset[1][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8fb132c4-1df5-4511-b44f-b218b1544f08",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'm'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m images,labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mtype\u001b[39m(labels)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[18], line 18\u001b[0m, in \u001b[0;36mAnimalDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m,index):\n\u001b[0;32m---> 18\u001b[0m     PIL_IMAGE \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimg_paths\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mresize(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize_of_images)\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;66;03m# In pytorch we use torch tensors, ToTensor transform transforms the PIL image \u001b[39;00m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;66;03m# to Torch tensor.\u001b[39;00m\n\u001b[1;32m     21\u001b[0m     TENSOR_IMAGE \u001b[38;5;241m=\u001b[39m transform(PIL_IMAGE)\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/PIL/Image.py:2953\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   2950\u001b[0m     filename \u001b[38;5;241m=\u001b[39m fp\n\u001b[1;32m   2952\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename:\n\u001b[0;32m-> 2953\u001b[0m     fp \u001b[38;5;241m=\u001b[39m \u001b[43mbuiltins\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2954\u001b[0m     exclusive_fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   2956\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'm'"
     ]
    }
   ],
   "source": [
    "images,labels = next(iter(train_loader))\n",
    "type(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef71e7be-97c0-4eca-87a1-3ad6b95e3157",
   "metadata": {},
   "outputs": [],
   "source": [
    "images,labels = iter(train_loader).next()\n",
    "\n",
    "fig, axis = plt.subplots(3, 5, figsize=(15, 10))\n",
    "for i, ax in enumerate(axis.flat):\n",
    "    with torch.no_grad():\n",
    "        npimg = images[i].numpy()\n",
    "        npimg = np.transpose(npimg, (1, 2, 0))\n",
    "        label = label_map[int(labels[i])]\n",
    "        ax.imshow(npimg)\n",
    "        ax.set(title = f\"{label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27896c90-77be-45e6-8da1-005ed42bbf1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(CNN,self).__init__()\n",
    "        # First we'll define our layers\n",
    "        self.conv1 = nn.Conv2d(3,32,kernel_size=3,stride=2,padding=1)\n",
    "        self.conv2 = nn.Conv2d(32,64,kernel_size=3,stride=2,padding=1)\n",
    "        self.batchnorm1 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(64,128,kernel_size=3,stride=2,padding=1)\n",
    "        self.batchnorm2 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        self.conv4 = nn.Conv2d(128,256,kernel_size=3,stride=2,padding=1)\n",
    "        self.batchnorm3 = nn.BatchNorm2d(256)\n",
    "        \n",
    "        self.maxpool = nn.MaxPool2d(2,2)\n",
    "        \n",
    "        self.fc1 = nn.Linear(256 * 2 * 2,512)\n",
    "        self.fc2 = nn.Linear(512,3)\n",
    "        \n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.batchnorm1(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.batchnorm2(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = self.batchnorm3(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = x.view(-1, 256 * 2 * 2)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.log_softmax(x,dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d024435-0742-4e6d-a03c-1ac52a254746",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed691d7c-412b-4a0d-85b5-e0484cc840d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173a065a-a355-4a8e-85a7-2bb6f715fb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98d86e0-d8bb-4729-984a-581e31812398",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.RMSprop(model.parameters(),lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01314ef3-6ba0-476b-a0e5-32a54538c5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH_NUMBER = 5\n",
    "TRAIN_LOSS = []\n",
    "TRAIN_ACCURACY = []\n",
    "\n",
    "for epoch in range(1,EPOCH_NUMBER+1):\n",
    "    epoch_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for data_,target_ in train_loader:\n",
    "        # We have to one hot encode our labels.\n",
    "        target_ =target_.to(device)\n",
    "        data_ = data_.to(device)\n",
    "        \n",
    "        # Cleaning the cached gradients if there are\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Getting train decisions and computing loss.\n",
    "        outputs = model(data_)\n",
    "        loss = criterion(outputs,target_)\n",
    "        \n",
    "        # Backpropagation and optimizing.\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Computing statistics.\n",
    "        epoch_loss = epoch_loss + loss.item()\n",
    "        _,pred = torch.max(outputs,dim=1)\n",
    "        correct = correct + torch.sum(pred == target_).item()\n",
    "        total += target_.size(0)\n",
    "    \n",
    "    # Appending stats to the lists.\n",
    "    TRAIN_LOSS.append(epoch_loss)\n",
    "    TRAIN_ACCURACY.append(100 * correct / total)\n",
    "    print(f\"Epoch {epoch}: Accuracy: {100 * correct/total}, Loss: {epoch_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9ea030-5a4f-46d0-8031-efa1ef6e6886",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(6,4))\n",
    "plt.plot(range(EPOCH_NUMBER),TRAIN_LOSS,color=\"blue\",label=\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.subplots(figsize=(6,4))\n",
    "plt.plot(range(EPOCH_NUMBER),TRAIN_ACCURACY,color=\"green\",label=\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f3f334-f93e-47e6-8bed-c94944cecc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_val_loss = 0.0\n",
    "total_true = 0\n",
    "total = len(test_sampler)\n",
    "\n",
    "# When we're not working with gradients and backpropagation\n",
    "# we use torch.no_grad() utility.\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    for data_,target_ in validation_loader:\n",
    "        data_ = data_.to(device)\n",
    "        target_ = target_.to(device)\n",
    "        \n",
    "        outputs = model(data_)\n",
    "        loss = criterion(outputs,target_).item()\n",
    "        _,preds = torch.max(outputs,dim=1)\n",
    "        total_val_loss += loss\n",
    "        true = torch.sum(preds == target_).item()\n",
    "        total_true += true\n",
    "\n",
    "validation_accuracy = round(100 * total_true / total,2)\n",
    "print(f\"Validation accuracy: {validation_accuracy}%\")\n",
    "print(f\"Validation loss: {round(total_val_loss,2)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5aa7114-90d5-4b42-b499-ae13b7377f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
